{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC + IF + SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from numpy import mean\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "#from autoimpute.imputations import MultipleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report,confusion_matrix, precision_recall_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Updated_UKBioBank.xlsx\")\n",
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific to UK BioBank dataset\n",
    "class PreProcessing:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def cleaning(self):\n",
    "        new_df = self.df.copy()\n",
    "        new_df = new_df.iloc[:, new_df.columns != 'QTrest']\n",
    "        new_df = new_df[~((new_df['AF']==1) & (new_df['Arr']==1))] # remove overlaps\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "        new_df = pd.DataFrame(imp.fit_transform(new_df), columns=new_df.columns)\n",
    "        new_df.loc[(new_df['Arr'] == 1) | (new_df['AF'] == 1), 'Disease'] = 1\n",
    "        new_df.loc[(new_df['Arr'] == 0) & (new_df['AF'] == 0), 'Disease'] = 0\n",
    "        self.df = new_df\n",
    "        return self.df\n",
    "    \n",
    "    def getTrainTestSet(self):\n",
    "        new_df = self.cleaning()\n",
    "        choosing_samples_AF = new_df[new_df['AF'] == 1]\n",
    "        choosing_samples_Arr =  new_df[new_df['Arr'] == 1]\n",
    "        healthy_samples = new_df[new_df['Disease'] == 0]\n",
    "        \n",
    "        AF_x = choosing_samples_AF.loc[:,choosing_samples_AF.columns != 'Disease']\n",
    "        AF_y = choosing_samples_AF['Disease']\n",
    "        \n",
    "        Arr_x = choosing_samples_Arr.loc[:,choosing_samples_Arr.columns != 'Disease']\n",
    "        Arr_y = choosing_samples_Arr['Disease']\n",
    "        \n",
    "        healthy_x = healthy_samples.loc[:,healthy_samples.columns != 'Disease']\n",
    "        healthy_y = healthy_samples['Disease']\n",
    "        \n",
    "        \n",
    "        AF_X_train, AF_X_test, AF_y_train, AF_y_test = train_test_split(AF_x, AF_y, test_size=0.1)\n",
    "        Arr_X_train, Arr_X_test, Arr_y_train, Arr_y_test = train_test_split(Arr_x, Arr_y, test_size=0.1)\n",
    "        healthy_X_train, healthy_X_test, healthy_y_train, healthy_y_test = train_test_split(healthy_x, healthy_y, test_size=0.01)\n",
    "        \n",
    "        X_test_df = pd.concat([AF_X_test,Arr_X_test,healthy_X_test])\n",
    "        y_test_df = pd.concat([AF_y_test,Arr_y_test,healthy_y_test])\n",
    "        X_train_df = pd.concat([AF_X_train,Arr_X_train,healthy_X_train])\n",
    "        y_train_df = pd.concat([AF_y_train,Arr_y_train,healthy_y_train])\n",
    "        X_train_df['Disease'] = y_train_df\n",
    "        X_test_df['Disease'] = y_test_df\n",
    "        self.df = X_train_df\n",
    "        return self.df, X_test_df\n",
    "    \n",
    "    # remving outliers using Isolation Forest\n",
    "    def OutlierRemoval(self):\n",
    "        new_df, X_test_df = self.getTrainTestSet()\n",
    "        new_df_healthy = new_df[new_df['Disease'] == 0]\n",
    "        new_df_disease = new_df[new_df['Disease'] == 1]\n",
    "        data = new_df_healthy.drop(columns=['AF', 'Arr']).values\n",
    "        X , y = data[:, :-1], data[:, -1]\n",
    "        iso = IsolationForest(random_state=4, contamination=0.2)\n",
    "        yhat = iso.fit_predict(X)\n",
    "        # select all rows that are not outliers\n",
    "        mask = yhat != -1\n",
    "        afterX, aftery = X[mask, :], y[mask]\n",
    "        no_outliers = pd.DataFrame(afterX, columns=new_df_healthy.drop(columns=['AF', 'Arr', 'Disease']).columns)\n",
    "        no_outliers['Disease'] = aftery\n",
    "        df1 = new_df_healthy.copy()\n",
    "        df1 = new_df_healthy.set_index('eid')\n",
    "        df2 = no_outliers.copy()\n",
    "        df2 = no_outliers.set_index('eid')\n",
    "        final_healthy_df = pd.merge(df1, df2, how='inner')\n",
    "        final_healthy_df['eid'] = df2.index\n",
    "        final_df = pd.concat([final_healthy_df, new_df_disease])\n",
    "        self.df = final_df\n",
    "        return self.df, X_test_df\n",
    "    \n",
    "    def DataAugmentation(self):\n",
    "        new_df, X_test_df = self.OutlierRemoval()\n",
    "        healthy_df = new_df[new_df['Disease'] == 0]\n",
    "        AF_data = new_df.drop(columns=['Arr'], axis=1) # data with only the AF label\n",
    "        Arr_data = new_df.drop(columns=['AF'], axis=1) # data with only the Arr label\n",
    "        arr_smote_x = Arr_data.loc[:,Arr_data.columns != 'Arr']\n",
    "        arr_smote_y = Arr_data['Arr']\n",
    "\n",
    "        oversample_arr = SMOTE(sampling_strategy=0.5)\n",
    "        arr_smote_x, arr_smote_y = oversample_arr.fit_resample(arr_smote_x, arr_smote_y)\n",
    "        af_smote_x = AF_data.loc[:,AF_data.columns != 'AF']\n",
    "        af_smote_y = AF_data['AF']\n",
    "\n",
    "        oversample_af = SMOTE(sampling_strategy=0.5)\n",
    "        af_smote_x, af_smote_y = oversample_af.fit_resample(af_smote_x, af_smote_y)\n",
    "        arr_smote_x['Arr'] = arr_smote_y\n",
    "        af_smote_x['AF'] = af_smote_y\n",
    "        AF_only = af_smote_x[af_smote_x['AF'] == 1] # data with only positive labels of AF\n",
    "        Arr_only = arr_smote_x[arr_smote_x['Arr'] ==1] # data with only positive labels of Arr\n",
    "        with_smote_df = pd.concat([Arr_only,AF_only,healthy_df])\n",
    "        \n",
    "        with_smote_df.loc[(with_smote_df['Arr'] == 1) | (with_smote_df['AF'] == 1), 'Disease'] = 1\n",
    "        with_smote_df.loc[(with_smote_df['Arr'] == 0) & (with_smote_df['AF'] == 0), 'Disease'] = 0\n",
    "        self.df = with_smote_df\n",
    "        return self.df, X_test_df\n",
    "    def finalCleanedDf(self):\n",
    "        new_df, X_test_df = self.DataAugmentation()\n",
    "        new_df.loc[(new_df['Arr'] == 1) & (new_df['Disease'] == 1), 'AF'] = 0\n",
    "        new_df.loc[(new_df['AF'] == 1) & (new_df['Disease'] == 1), 'Arr'] = 0\n",
    "        self.df = new_df\n",
    "        return self.df, X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = PreProcessing(df)\n",
    "train_df, test_df= instance.finalCleanedDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    42052\n",
       "0.0    41224\n",
       "Name: Disease, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Disease'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    521\n",
       "1.0    186\n",
       "Name: Disease, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Disease'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    690\n",
       "1.0     17\n",
       "Name: Arr, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Arr'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['eid', 'AF', 'Arr', 'Disease'])\n",
    "y_train = train_df['Disease']\n",
    "\n",
    "\n",
    "X_test = test_df.drop(columns=['eid', 'AF', 'Arr', 'Disease'])\n",
    "y_test = test_df['Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "\n",
    "model1 = xgb.XGBClassifier(objective='binary:logistic', subsample=0.75,  \n",
    "                          n_estimators=100, colsample_bytree = 0.99, learning_rate = 1,\n",
    "                max_depth = 10,  gamma=5, alpha = 1, seed=123, use_label_encoder=False, eval_metric='error')\n",
    "\n",
    "estimators.append(('XGBoost', model1))\n",
    "estimators.append(('bayes', GaussianNB()))\n",
    "estimators.append(('GradientDescent' , SGDClassifier(loss=\"modified_huber\", penalty=\"elasticnet\", max_iter=500)))\n",
    "ensemble = VotingClassifier(estimators, voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate score\n",
    "score = f1_score(y_test, y_pred)\n",
    "print('F1 Score: %.3f' % score)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print('Precision: %.3f' % precision)\n",
    "\n",
    "\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ensemble, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "               | Positive Prediction | Negative Prediction\n",
    "Positive Class | True Positive (TP)  | False Negative (FN)\n",
    "Negative Class | False Positive (FP) | True Negative (TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['Disease'] ==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondStageProcessing:\n",
    "    def __init__(self, traindf, testdf):\n",
    "        self.traindf = traindf\n",
    "        self.testdf = testdf\n",
    "        \n",
    "    def prcoessing(self):\n",
    "        new_df = self.traindf.copy()\n",
    "        test_df = self.testdf.copy()\n",
    "        X_train_2ndStage = new_df.copy()\n",
    "        X_train_2ndStage = X_train_2ndStage[X_train_2ndStage['Disease']==1]\n",
    "        X_train_2ndStage['label'] = 0\n",
    "        X_train_2ndStage.loc[(X_train_2ndStage['Arr'] == 0) & (X_train_2ndStage['AF'] == 1), 'label'] = 0\n",
    "        X_train_2ndStage.loc[(X_train_2ndStage['Arr'] == 1) & (X_train_2ndStage['AF'] == 0), 'label'] = 1\n",
    "        X_train_2ndStage = X_train_2ndStage.drop(columns=['AF', 'Arr', 'Disease'])\n",
    "        X_test_2ndStage = test_df.copy()\n",
    "        X_test_2ndStage = X_test_2ndStage[X_test_2ndStage['Disease']==1]\n",
    "        X_test_2ndStage['label'] = 0\n",
    "        X_test_2ndStage.loc[(X_test_2ndStage['Arr'] == 0) & (X_test_2ndStage['AF'] == 1), 'label'] = 0\n",
    "        X_test_2ndStage.loc[(X_test_2ndStage['Arr'] == 1) & (X_test_2ndStage['AF'] == 0), 'label'] = 1\n",
    "        self.traindf = X_train_2ndStage\n",
    "        self.testdf = X_test_2ndStage\n",
    "        return self.traindf, self.testdf\n",
    "    \n",
    "    # remving outliers using Isolation Forest\n",
    "    def OutlierRemoval(self):\n",
    "        train_df, X_test_df = self.prcoessing()\n",
    "        data = train_df.to_numpy()\n",
    "        X , y = data[:, :-1], data[:, -1]\n",
    "\n",
    "        iso = IsolationForest(contamination=0.1)\n",
    "        yhat = iso.fit_predict(X)\n",
    "        # select all rows that are not outliers\n",
    "        mask = yhat != -1\n",
    "        afterX, aftery = X[mask, :], y[mask]\n",
    "        unhealthy_df_no_outliers = pd.DataFrame(afterX,  columns = train_df.drop(columns=['label']).columns)\n",
    "        unhealthy_df_no_outliers['label'] = aftery\n",
    "        unhealthy_df_no_outliers['AF'] = 0\n",
    "        unhealthy_df_no_outliers['Arr'] = 0\n",
    "        unhealthy_df_no_outliers.loc[(unhealthy_df_no_outliers['label'] == 0), 'AF'] = 1\n",
    "        unhealthy_df_no_outliers.loc[(unhealthy_df_no_outliers['label'] == 1), 'Arr'] = 1\n",
    "        self.traindf = unhealthy_df_no_outliers\n",
    "        return self.traindf, self.testdf \n",
    "\n",
    "    def DataAugmentation(self):\n",
    "        train_df, X_test_df = self.OutlierRemoval()\n",
    "        arr_x = train_df.loc[:,train_df.columns != 'Arr']\n",
    "        arr_y = train_df['Arr']\n",
    "        AF_data = train_df.drop(columns=['Arr'], axis=1) # data with only the AF label\n",
    "        Arr_data = train_df.drop(columns=['AF'], axis=1) # data with only the Arr label\n",
    "        arr_smote_x = Arr_data.loc[:,Arr_data.columns != 'Arr']\n",
    "        arr_smote_y = Arr_data['Arr']\n",
    "\n",
    "        oversample_arr = SMOTE(sampling_strategy='auto')\n",
    "        arr_smote_x, arr_smote_y = oversample_arr.fit_resample(arr_smote_x, arr_smote_y)\n",
    "        af_smote_x = AF_data.loc[:,AF_data.columns != 'AF']\n",
    "        af_smote_y = AF_data['AF']\n",
    "\n",
    "        oversample_af = SMOTE(sampling_strategy='auto')\n",
    "        af_smote_x, af_smote_y = oversample_af.fit_resample(af_smote_x, af_smote_y)\n",
    "        arr_smote_x['Arr'] = arr_smote_y\n",
    "        af_smote_x['AF'] = af_smote_y\n",
    "        AF_only = af_smote_x[af_smote_x['AF'] == 1] # data with only positive labels of AF\n",
    "        Arr_only = arr_smote_x[arr_smote_x['Arr'] ==1] # data with only positive labels of Arr\n",
    "        with_smote_df = pd.concat([Arr_only,AF_only])\n",
    "        \n",
    "        with_smote_df.loc[(with_smote_df['Arr'] == 1) & (with_smote_df['AF'] == 0), 'label'] = 1\n",
    "        with_smote_df.loc[(with_smote_df['Arr'] == 0) & (with_smote_df['AF'] == 1), 'label'] = 0\n",
    "        self.df = with_smote_df\n",
    "        return self.df, X_test_df\n",
    "\n",
    "       \n",
    "    def finalCleanedDf(self):\n",
    "        new_df, X_test_df = self.DataAugmentation()\n",
    "        \n",
    "        new_df.loc[(new_df['label'] == 0), 'AF'] = 1\n",
    "        new_df.loc[(new_df['label'] == 0), 'Arr'] = 0\n",
    "        new_df.loc[(new_df['label'] == 1), 'Arr'] = 1\n",
    "        new_df.loc[(new_df['label'] == 1), 'AF'] = 0\n",
    "        self.df = new_df\n",
    "        return self.df, X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = SecondStageProcessing(train_df, test_df)\n",
    "train2_df, test2_df= instance.finalCleanedDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = train2_df.drop(columns=['eid', 'AF', 'Arr', 'label'])\n",
    "y2_train = train2_df['label']\n",
    "\n",
    "X_test = test2_df.drop(columns=['eid', 'AF', 'Arr', 'Disease', 'label'])\n",
    "y_test = test2_df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimators = []\n",
    "\n",
    "model1 = xgb.XGBClassifier(objective='binary:logistic', subsample=0.75,  \n",
    "                          n_estimators=100, colsample_bytree = 0.99, learning_rate = 1,\n",
    "                max_depth = 10,  gamma=5, alpha = 1, seed=123, use_label_encoder=False, eval_metric='error')\n",
    "\n",
    "estimators.append(('XGBoost', model1))\n",
    "estimators.append(('bayes', GaussianNB()))\n",
    "estimators.append(('GradientDescent' , SGDClassifier(loss=\"modified_huber\", penalty=\"elasticnet\", max_iter=500)))\n",
    "ensemble = VotingClassifier(estimators, voting = 'soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X2_train, y2_train)\n",
    "y_pred_2 = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, y_pred_2)\n",
    "print('f1_score: %.3f' % f1_score)\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred_2)\n",
    "print('Precision: %.3f' % precision)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred_2, average='binary')\n",
    "print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ensemble, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
